


# Policies for LLM Use in the Workplace

As large language models become more embedded in daily workflows, it’s time for businesses to clarify their expectations and boundaries. Smart policy isn’t about locking things down—it’s about enabling good use with clear guardrails.

## Why Policy Matters

Without policy, LLMs create:
- Ambiguity around data sharing
- Uneven usage across teams
- Risk of employees using personal accounts for sensitive work
- Unclear expectations on accuracy and oversight

A good policy reduces risk, increases adoption, and builds trust.

## Core Principles to Cover

1. **Privacy and Confidentiality**
   - Prohibit use of sensitive data in public LLM tools unless explicitly approved.
   - Clarify which data types are allowed with which tools.

2. **Approved Tools and Accounts**
   - Require business accounts for any LLM-related work.
   - Discourage or block personal account use for company content.

3. **Disclosure and Oversight**
   - Encourage labeling or logging of LLM-assisted content.
   - Require human review before LLM-generated work is published or acted on.

4. **Security**
   - Review provider policies (e.g., OpenAI, Microsoft Copilot) to understand retention and training risks.
   - Prefer vendors offering data isolation, encryption, and opt-outs from model training.

5. **Training and Literacy**
   - Teach staff what LLMs can and can’t do.
   - Provide guidelines on prompt safety, context limits, and bias detection.

## Cultural Signals

A policy isn’t just rules—it’s a signal. It tells employees:
- “We expect you to use this.”
- “We’ll help you do it well.”
- “We care about getting this right.”

Policies that only emphasize fear will lead to shadow use. Policies that support smart adoption create empowered teams.

## Summary

Don’t wait until something breaks to define your rules. Establish a thoughtful policy that embraces experimentation while protecting the business.

Your minions need a user manual. Write one that supports trust, clarity, and good decisions.
